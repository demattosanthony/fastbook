{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using full mnist dataset and creating neural net from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import inflect\n",
    "from tqdm import tqdm\n",
    "from fastai.data.core import DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/0'),Path('training/1'),Path('training/2'),Path('training/3'),Path('training/4'),Path('training/5'),Path('training/6'),Path('training/7'),Path('training/8'),Path('training/9')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through training data and storing it in numbers dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "numbers = {}\n",
    "for i in (path/'training').ls().sorted():\n",
    "    num = str(i)[-1]\n",
    "    for n in num:\n",
    "        val = p.number_to_words(n)\n",
    "        numbers[val] = (path/'training/{}'.format(n)).ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_tensors = {}\n",
    "for key, val in numbers.items():\n",
    "    numbers_tensors[key] = [tensor(Image.open(o)) for o in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6742, 5958)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numbers_tensors['one']),len(numbers_tensors['two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAADjUlEQVR4nO2cTW8bRQCGn5n1rp1NHKeYhNpJ00iEBoIoh6IeOHBAfEicigSCG4eKA3d+AEfEjV+AxBFOPUIlQIIDUpFaCoFCQ1NUghs7/iiJ15+7w6FEKVO3t86MqnmOa0vv60ezO96d0QqlFJ5DpO0CruGFaHghGl6Ihheikbvfhy/LNx/aKeh89rmYdNyPEA0vRMML0fBCNLwQDS9E477T7gMNXlmmtzqPyklUAIV6n6DeQe13Ud2EbDiCLDXfy3jif/z1+hLvvXuOY1GTlVybty+eJfxyidLWmKk/mgSNJmnnlvFe1k6ZNA/rhW3Ww12ejqZ4fnGL9smUxrMh7ecWYPGolV7WRggKMiXJgFRlfFT9ig+Onufc/hrftNfY/GSN8ob5WvaE3EGGIhYRcXB71CSlPFfiJ610cW6WeSZMeGP2MsOinXznhEghCG3mW8x2Ei9EwwvRcFZIllcEcyVEGBnNdU5IgCAQgnGsoLKAnJk2mu+cEIkkLyRyscfu6UehumA43zFCERCLiFdWr9B6tcf+aslovnNCDpAiQwBMfBT8AHPNxrmPF6LhhWg4K6QcdjlS6jKckchiEZEzc2PurJBK1GGp2GFYFMjSLCKfN5LrrJBTheucWbhItwrjpTLC0B80Z4WcjALeKtYYVkYklSnEdGwk15qQ6Jbis9ZpLg2qtipMxJ6QPcX3teP81q/YqjARa0Jmtof0fijzdeOErQoTsTdCav9Q3kj5s3HEVoWJOHtRtYUXouGFaLgvRIAy2NJpIRLJsaUm9VOSYXXOUKYtlEJkoDLJSKVkZBO/9kSpQbrcZzRrZvnKnpBGi+LPDcKNmPdrL/BFMvlR4WuPXObMUz+SzD/kd7vZ3h7q7x3iuuLCzjKbg8nbH9ajm7xY+oWxmVsZe0JUmpL1+gQDSAYhSWZ2/eVeWNwfokClyLFiNMzRz2wucR9ifZbJd1LUjZjN7vw9L6wmsS4kl6REbUG7H5MqdZeUROVojmcQhvbfWd9BFP10nePbc/y6UmFwYkyeHPKOtZiPb77Ed7+vsnxjbKSPdSFpswXNFlH9MS4MSkzLAQVx+8enCC7tLJLfKhB1ukb6WBdywOOf7vDht++gBCAOh0h1t4fs1KDexMRZ44yQ9Oo1wqvX7jquwIiIA6xfVF3DC9HwQjS8EA0vRMML0RD+ZQj/x48QDS9EwwvR8EI0vBANL0TjX8E5BcMcUxKSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(numbers_tensors['one'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6742, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_numbers = {}\n",
    "for key, val in numbers_tensors.items():\n",
    "    stacked_numbers[key] = torch.stack(numbers_tensors[key]).float()/255\n",
    "stacked_numbers['one'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_numbers = {}\n",
    "for i in tqdm((path/'testing').ls().sorted()):\n",
    "    num = str(i)[-1]\n",
    "    for n in num:\n",
    "        val = p.number_to_words(n)\n",
    "        valid_numbers[val] = (path/'testing/{}'.format(n)).ls().sorted()\n",
    "        valid_numbers[val] = torch.stack([tensor(Image.open(o)) for o in numbers[val]]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6742, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_numbers['one'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([val for key, val in stacked_numbers.items()]).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(pos,num):\n",
    "    tns = torch.zeros([num,10])\n",
    "    tns[:,pos] = 1\n",
    "    return tns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = [create_labels(i,len(stacked_numbers[j])) for i,j in zip(range(len(stacked_numbers)),stacked_numbers)]\n",
    "train_y = torch.cat(train_y)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x,train_y))\n",
    "x,y = dset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([val for key,val in valid_numbers.items()]).view(-1,28*28)\n",
    "valid_y = [create_labels(i,len(valid_numbers[j])) for i,j in zip(range(len(valid_numbers)),valid_numbers)]\n",
    "valid_y = torch.cat(valid_y)\n",
    "valid_dset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginnning to define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = init_params(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 10]), torch.Size([10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape,bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.4666, -11.3850,  -6.5017,  ...,   3.3681,   4.7082,  -2.9032],\n",
       "        [  0.2836, -14.8603,  -6.2512,  ...,  15.5978,   0.4971,  -4.8285],\n",
       "        [-13.6844, -10.0672,  -1.2629,  ...,  23.7001,   4.9805,  -3.2281],\n",
       "        ...,\n",
       "        [ -1.4088,   1.0843,   1.0424,  ...,   5.8106,  -8.7747,  -7.4298],\n",
       "        [-17.6492, -10.5786,   6.3164,  ...,   8.9424,  -2.8318,   2.6672],\n",
       "        [ -8.8005,  -5.3353,   1.4041,  ...,  17.3704,   3.2372,   3.4278]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(xb): return xb@weights + bias\n",
    "preds = linear1(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    #predictions = predictions.softmax(dim=1)\n",
    "    target = torch.max(targets,1).indices\n",
    "    #print(predictions,target)\n",
    "    return nn.CrossEntropyLoss()(predictions,target)\n",
    "    #loss = -(targets * predictions.log()).sum() / len(predictions)\n",
    "    #return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.9326, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(preds,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 784]), torch.Size([128, 10]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=128)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=128)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loss on small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_x[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linear1(batch)\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.9461, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds,train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_accuracy(linear1(batch), train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions to implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb,yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds,yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb,yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.softmax(dim=1)\n",
    "    predVal, predMax = torch.max(preds.data,1)\n",
    "    tarVal, tarMax = torch.max(yb,1)\n",
    "    correct = predMax == tarMax\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "params = weights,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1242 0.1456 0.1691 0.1903 0.2106 0.2297 0.2466 0.2621 0.278 0.2935 0.3088 0.324 0.3399 0.3546 0.3702 0.3857 0.4019 0.4173 0.4304 0.4433 0.4557 0.467 0.478 0.4874 0.4969 0.5059 0.5137 0.5228 0.5303 0.5379 0.5443 0.5504 0.5569 0.5631 0.5688 0.575 0.5802 0.5856 0.5901 0.5951 "
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr):\n",
    "        self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.lr\n",
    "            \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0609"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2506 0.4274 0.5557 0.6365 0.6876 0.7227 0.747 0.7647 0.7778 0.7883 0.7975 0.8045 0.8108 0.8167 0.8211 0.8252 0.8291 0.8321 0.8354 0.8384 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2715 0.4668 0.5737 0.64 0.6867 0.7205 0.7461 0.7645 0.7791 0.7908 0.7995 0.8072 0.8133 0.8188 0.8235 0.8278 0.8313 0.8348 0.8375 0.8402 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,10)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.107316</td>\n",
       "      <td>1.970764</td>\n",
       "      <td>0.310450</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.686857</td>\n",
       "      <td>1.714210</td>\n",
       "      <td>0.457583</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.478348</td>\n",
       "      <td>1.501121</td>\n",
       "      <td>0.562433</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.338799</td>\n",
       "      <td>1.338413</td>\n",
       "      <td>0.631033</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.234081</td>\n",
       "      <td>1.214339</td>\n",
       "      <td>0.676317</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.151810</td>\n",
       "      <td>1.117783</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.085409</td>\n",
       "      <td>1.040892</td>\n",
       "      <td>0.735100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.030701</td>\n",
       "      <td>0.978351</td>\n",
       "      <td>0.753633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.984844</td>\n",
       "      <td>0.926526</td>\n",
       "      <td>0.769050</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.945830</td>\n",
       "      <td>0.882884</td>\n",
       "      <td>0.781017</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding NonLinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb):\n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = xb@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 30]), torch.Size([30]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)\n",
    "w1.shape,b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.323608</td>\n",
       "      <td>0.318803</td>\n",
       "      <td>0.905283</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.322998</td>\n",
       "      <td>0.318005</td>\n",
       "      <td>0.905567</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.322391</td>\n",
       "      <td>0.317212</td>\n",
       "      <td>0.905867</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321786</td>\n",
       "      <td>0.316427</td>\n",
       "      <td>0.906133</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.321180</td>\n",
       "      <td>0.315646</td>\n",
       "      <td>0.906450</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.320573</td>\n",
       "      <td>0.314869</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.319973</td>\n",
       "      <td>0.314098</td>\n",
       "      <td>0.906817</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.319385</td>\n",
       "      <td>0.313331</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.318778</td>\n",
       "      <td>0.312573</td>\n",
       "      <td>0.907217</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.318194</td>\n",
       "      <td>0.311809</td>\n",
       "      <td>0.907417</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.317603</td>\n",
       "      <td>0.311053</td>\n",
       "      <td>0.907633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>0.310295</td>\n",
       "      <td>0.907850</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.316445</td>\n",
       "      <td>0.309546</td>\n",
       "      <td>0.908117</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.315854</td>\n",
       "      <td>0.308801</td>\n",
       "      <td>0.908317</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.315278</td>\n",
       "      <td>0.308054</td>\n",
       "      <td>0.908550</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.314696</td>\n",
       "      <td>0.307315</td>\n",
       "      <td>0.908850</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.314129</td>\n",
       "      <td>0.306577</td>\n",
       "      <td>0.909167</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.313553</td>\n",
       "      <td>0.305844</td>\n",
       "      <td>0.909450</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.305115</td>\n",
       "      <td>0.909667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.312416</td>\n",
       "      <td>0.304388</td>\n",
       "      <td>0.909767</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.311848</td>\n",
       "      <td>0.303672</td>\n",
       "      <td>0.910117</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>0.302954</td>\n",
       "      <td>0.910317</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.310722</td>\n",
       "      <td>0.302242</td>\n",
       "      <td>0.910517</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.310172</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>0.910767</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.309630</td>\n",
       "      <td>0.300819</td>\n",
       "      <td>0.910983</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.309072</td>\n",
       "      <td>0.300111</td>\n",
       "      <td>0.911183</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.308512</td>\n",
       "      <td>0.299409</td>\n",
       "      <td>0.911433</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.298712</td>\n",
       "      <td>0.911633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.307392</td>\n",
       "      <td>0.298016</td>\n",
       "      <td>0.911983</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.306838</td>\n",
       "      <td>0.297326</td>\n",
       "      <td>0.912233</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.306299</td>\n",
       "      <td>0.296631</td>\n",
       "      <td>0.912450</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.305746</td>\n",
       "      <td>0.295946</td>\n",
       "      <td>0.912633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.305190</td>\n",
       "      <td>0.295266</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.304651</td>\n",
       "      <td>0.294587</td>\n",
       "      <td>0.913150</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.304120</td>\n",
       "      <td>0.293908</td>\n",
       "      <td>0.913317</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.303587</td>\n",
       "      <td>0.293232</td>\n",
       "      <td>0.913550</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.303055</td>\n",
       "      <td>0.292560</td>\n",
       "      <td>0.913733</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.302515</td>\n",
       "      <td>0.291896</td>\n",
       "      <td>0.913883</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.301975</td>\n",
       "      <td>0.291238</td>\n",
       "      <td>0.914117</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.301437</td>\n",
       "      <td>0.290576</td>\n",
       "      <td>0.914400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.300902</td>\n",
       "      <td>0.289919</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.300370</td>\n",
       "      <td>0.289265</td>\n",
       "      <td>0.914633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.299843</td>\n",
       "      <td>0.288614</td>\n",
       "      <td>0.914833</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.299306</td>\n",
       "      <td>0.287966</td>\n",
       "      <td>0.915050</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.298789</td>\n",
       "      <td>0.287315</td>\n",
       "      <td>0.915250</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.298283</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.915417</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.297773</td>\n",
       "      <td>0.286026</td>\n",
       "      <td>0.915567</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.297247</td>\n",
       "      <td>0.285389</td>\n",
       "      <td>0.915817</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.296734</td>\n",
       "      <td>0.284752</td>\n",
       "      <td>0.916017</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.296235</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>0.916167</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.295728</td>\n",
       "      <td>0.283480</td>\n",
       "      <td>0.916333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.282853</td>\n",
       "      <td>0.916517</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.294692</td>\n",
       "      <td>0.282230</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.294183</td>\n",
       "      <td>0.281607</td>\n",
       "      <td>0.916917</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.293677</td>\n",
       "      <td>0.280986</td>\n",
       "      <td>0.917150</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.293171</td>\n",
       "      <td>0.280369</td>\n",
       "      <td>0.917367</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.292672</td>\n",
       "      <td>0.279754</td>\n",
       "      <td>0.917717</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.292176</td>\n",
       "      <td>0.279138</td>\n",
       "      <td>0.917867</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.918133</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.291162</td>\n",
       "      <td>0.277921</td>\n",
       "      <td>0.918283</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.290657</td>\n",
       "      <td>0.277317</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.290155</td>\n",
       "      <td>0.276717</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.289660</td>\n",
       "      <td>0.276115</td>\n",
       "      <td>0.919067</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.289165</td>\n",
       "      <td>0.275516</td>\n",
       "      <td>0.919233</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.288672</td>\n",
       "      <td>0.274918</td>\n",
       "      <td>0.919417</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.288183</td>\n",
       "      <td>0.274328</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.287685</td>\n",
       "      <td>0.273737</td>\n",
       "      <td>0.919817</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>0.273146</td>\n",
       "      <td>0.920067</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.286728</td>\n",
       "      <td>0.272556</td>\n",
       "      <td>0.920267</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.286251</td>\n",
       "      <td>0.271968</td>\n",
       "      <td>0.920550</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.285767</td>\n",
       "      <td>0.271386</td>\n",
       "      <td>0.920733</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.285273</td>\n",
       "      <td>0.270807</td>\n",
       "      <td>0.920850</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.284776</td>\n",
       "      <td>0.270233</td>\n",
       "      <td>0.921017</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.284295</td>\n",
       "      <td>0.269662</td>\n",
       "      <td>0.921167</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.283819</td>\n",
       "      <td>0.269089</td>\n",
       "      <td>0.921283</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.283335</td>\n",
       "      <td>0.268521</td>\n",
       "      <td>0.921450</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.282870</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.921633</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.267377</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.281932</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.922050</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.281464</td>\n",
       "      <td>0.266255</td>\n",
       "      <td>0.922217</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.280990</td>\n",
       "      <td>0.265696</td>\n",
       "      <td>0.922433</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.280520</td>\n",
       "      <td>0.265139</td>\n",
       "      <td>0.922650</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.280050</td>\n",
       "      <td>0.264584</td>\n",
       "      <td>0.922850</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.279569</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.923033</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.279086</td>\n",
       "      <td>0.263485</td>\n",
       "      <td>0.923233</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.278618</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.923350</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.278154</td>\n",
       "      <td>0.262394</td>\n",
       "      <td>0.923517</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.277699</td>\n",
       "      <td>0.261852</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.277240</td>\n",
       "      <td>0.261311</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.276771</td>\n",
       "      <td>0.260776</td>\n",
       "      <td>0.924067</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.276304</td>\n",
       "      <td>0.260241</td>\n",
       "      <td>0.924217</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.275838</td>\n",
       "      <td>0.259709</td>\n",
       "      <td>0.924300</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.275378</td>\n",
       "      <td>0.259176</td>\n",
       "      <td>0.924433</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.274931</td>\n",
       "      <td>0.258642</td>\n",
       "      <td>0.924583</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.274488</td>\n",
       "      <td>0.258109</td>\n",
       "      <td>0.924767</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.274041</td>\n",
       "      <td>0.257579</td>\n",
       "      <td>0.924867</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.273590</td>\n",
       "      <td>0.257054</td>\n",
       "      <td>0.925050</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.273149</td>\n",
       "      <td>0.256529</td>\n",
       "      <td>0.925117</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.272702</td>\n",
       "      <td>0.256010</td>\n",
       "      <td>0.925367</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.272242</td>\n",
       "      <td>0.255495</td>\n",
       "      <td>0.925583</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.271792</td>\n",
       "      <td>0.254981</td>\n",
       "      <td>0.925817</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.271351</td>\n",
       "      <td>0.254466</td>\n",
       "      <td>0.925950</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.270906</td>\n",
       "      <td>0.253955</td>\n",
       "      <td>0.926117</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.270462</td>\n",
       "      <td>0.253447</td>\n",
       "      <td>0.926233</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.270022</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.926367</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.269588</td>\n",
       "      <td>0.252434</td>\n",
       "      <td>0.926550</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.269150</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.268716</td>\n",
       "      <td>0.251427</td>\n",
       "      <td>0.926917</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.268265</td>\n",
       "      <td>0.250929</td>\n",
       "      <td>0.927067</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.267823</td>\n",
       "      <td>0.250433</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.267373</td>\n",
       "      <td>0.249940</td>\n",
       "      <td>0.927467</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.266944</td>\n",
       "      <td>0.249442</td>\n",
       "      <td>0.927567</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.266507</td>\n",
       "      <td>0.248951</td>\n",
       "      <td>0.927633</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.266072</td>\n",
       "      <td>0.248458</td>\n",
       "      <td>0.927717</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.265635</td>\n",
       "      <td>0.247972</td>\n",
       "      <td>0.928017</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.247486</td>\n",
       "      <td>0.928133</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.264759</td>\n",
       "      <td>0.247006</td>\n",
       "      <td>0.928300</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.264314</td>\n",
       "      <td>0.246529</td>\n",
       "      <td>0.928483</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.263882</td>\n",
       "      <td>0.246053</td>\n",
       "      <td>0.928650</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.263454</td>\n",
       "      <td>0.245575</td>\n",
       "      <td>0.928733</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.245099</td>\n",
       "      <td>0.928883</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.262617</td>\n",
       "      <td>0.244626</td>\n",
       "      <td>0.928950</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.244153</td>\n",
       "      <td>0.929067</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.243681</td>\n",
       "      <td>0.929217</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.261353</td>\n",
       "      <td>0.243211</td>\n",
       "      <td>0.929367</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.260932</td>\n",
       "      <td>0.242743</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.260515</td>\n",
       "      <td>0.242278</td>\n",
       "      <td>0.929833</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.260097</td>\n",
       "      <td>0.241814</td>\n",
       "      <td>0.929983</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.259693</td>\n",
       "      <td>0.241352</td>\n",
       "      <td>0.930183</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.259283</td>\n",
       "      <td>0.240892</td>\n",
       "      <td>0.930333</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.258880</td>\n",
       "      <td>0.240435</td>\n",
       "      <td>0.930517</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.258469</td>\n",
       "      <td>0.239980</td>\n",
       "      <td>0.930617</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.258057</td>\n",
       "      <td>0.239525</td>\n",
       "      <td>0.930717</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.257639</td>\n",
       "      <td>0.239074</td>\n",
       "      <td>0.930867</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.257241</td>\n",
       "      <td>0.238622</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.256845</td>\n",
       "      <td>0.238170</td>\n",
       "      <td>0.931283</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.256435</td>\n",
       "      <td>0.237721</td>\n",
       "      <td>0.931417</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.256026</td>\n",
       "      <td>0.237275</td>\n",
       "      <td>0.931500</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.255616</td>\n",
       "      <td>0.236829</td>\n",
       "      <td>0.931600</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.255202</td>\n",
       "      <td>0.236386</td>\n",
       "      <td>0.931717</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.254788</td>\n",
       "      <td>0.235948</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.254389</td>\n",
       "      <td>0.235509</td>\n",
       "      <td>0.931883</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.253991</td>\n",
       "      <td>0.235068</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.253604</td>\n",
       "      <td>0.234629</td>\n",
       "      <td>0.932067</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.253205</td>\n",
       "      <td>0.234192</td>\n",
       "      <td>0.932233</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.252816</td>\n",
       "      <td>0.233756</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.252422</td>\n",
       "      <td>0.233324</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.252025</td>\n",
       "      <td>0.232895</td>\n",
       "      <td>0.932600</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.251639</td>\n",
       "      <td>0.232467</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.251247</td>\n",
       "      <td>0.232039</td>\n",
       "      <td>0.932850</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(150,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b65e0ee20>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328500032424927"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.120724</td>\n",
       "      <td>0.138273</td>\n",
       "      <td>0.959700</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path, train='training', valid='testing')\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit(1,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
